---
title: DevOpsYoda â€“ AI Agent for DevOps
emoji: ğŸ§ 
colorFrom: green
colorTo: gray
sdk: gradio
sdk_version: "3.50.2"
app_file: app.py
pinned: false
---
# DevOpsYoda
> âš™ï¸ **Note**: The section above is required for Hugging Face Spaces deployment. It defines the Space metadata including title, emoji, colors, SDK type/version, and the entry-point script (`app.py`). Without it, the Space will fail to build.

# ğŸ§  DevOpsYoda â€“ AI Agent for DevOps Wisdom

**DevOpsYoda** is a smart AI assistant that simulates DevOps support by answering infrastructure-related questions, analyzing CI/CD config files, troubleshooting Kubernetes manifests, and more â€” all through natural language queries.

---

## ğŸš€ What This POC Demonstrates (AI Capabilities)

- ğŸ” **RAG-based Q&A** over:
  - Jenkinsfiles, GitHub Actions YAMLs
  - Kubernetes manifests (Helm, YAML)
  - Terraform and infra-as-code snippets
  - Markdown/PDF documentation
  - CI/CD logs and failure traces

- ğŸ§  **AI Assistant Capabilities**:
  - Explains DevOps config files
  - Diagnoses likely issues in YAML or Dockerfiles
  - Suggests remediations or security best practices
  - Summarizes pod or pipeline failure logs
  - Can be extended to generate code (e.g., Terraform)

- ğŸ¤– **Agentic Reasoning** *(via LangChain Agent)*:
  - Selects the right tool based on task (log summarizer, YAML linter, etc.)
  - Multi-step thinking possible with agent tools

---

## ğŸ§ª Enhanced POC Use Case: "Ask Me Anything About Your Infra"

A developer can interact with this chatbot to:
- Debug YAML misconfigurations
- Understand CI/CD automation flows
- Validate security posture of infra setup
- Generate code snippets or summaries
- Retrieve answers from private DevOps documentation

---

## ğŸ› ï¸ Tech Stack Used

| Layer           | Technology                                                 |
|----------------|-------------------------------------------------------------|
| LLM            | `mistralai/Mistral-7B-Instruct-v0.1` or `google/flan-t5-xl` |
| Embeddings     | `sentence-transformers/all-MiniLM-L6-v2`                    |
| RAG            | `LangChain + FAISS`                                         |
| Agentic Flow   | `LangChain Agents + Toolset`                                |
| UI             | `Gradio` (chat interface)                                   |
| Deployment     | `Hugging Face Spaces` (completely free tier)                |

---
ğŸ’¸ Cost and Hosting Information
ğŸ§  Inference
This POC does not use OpenAI. Instead, it leverages free, open-source LLMs hosted by Hugging Face, such as:
google/flan-t5-xl
mistralai/Mistral-7B-Instruct-v0.1

These models are loaded via the transformers library and run entirely within the Hugging Face Space using the CPU/GPU resources provided for free (within limits).
ğŸŸ¢ This means you are not charged for inference, as long as you stay within Hugging Face's free tier resource constraints.
âœ… No OpenAI key is required.

ğŸš€ Hosting
This project is deployed on Hugging Face Spaces using:
Gradio UI (chat interface)
requirements.txt for installing dependencies
README.md for metadata and configuration
app.py as the entry point
Hugging Face offers free Spaces with:
CPU (free tier)
16 GB of RAM
Automatic deployment on git push
Public or private repo settings
ğŸ’¡ Hugging Faceâ€™s free tier is sufficient for running this POC unless you require faster inference or GPU compute (available via paid options).

âœ… Summary
Component	Service	Free?	Notes
LLM Inference	Hugging Face models	âœ… Yes	No OpenAI API key or paid service used
Embeddings	Sentence Transformers	âœ… Yes	Using all-MiniLM-L6-v2 via Hugging Face, CPU-compatible
Vector Store	FAISS	âœ… Yes	Open-source and runs in memory within the Space
UI	Gradio	âœ… Yes	Free and easy to use in Hugging Face Spaces
Hosting	Hugging Face Spaces	âœ… Yes	Free for CPU usage, with basic memory and compute allocation
## ğŸ“ Data Folder Suggestions (`data/`)

Include these types of files to make the assistant functional:

- `jenkinsfile.txt`, `github-actions.yaml`  
- `terraform.tf`, `infra.md`  
- `k8s-deployment.yaml` (intentional issues welcome)  
- `pod-logs.txt`, `cicd-errors.log`  
- `best-practices.pdf`, `readme.md`  

---

## ğŸ’¬ Sample Prompts for Demo

- â€œWhy might this Kubernetes YAML fail to deploy?â€
- â€œExplain this GitHub Actions pipelineâ€
- â€œSummarize these CI/CD logsâ€
- â€œGenerate Terraform for an EC2 instanceâ€
- â€œWhatâ€™s wrong with this pod log?â€

---

## ğŸ“ Folder Structure

DevOpsYoda/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ data/
â”œâ”€â”€ jenkinsfile.txt
â”œâ”€â”€ k8s-deployment.yaml
â”œâ”€â”€ terraform-snippet.tf
â”œâ”€â”€ pod-log.txt
â””â”€â”€ best-practices.pdf


---

## ğŸ”§ Run Locally

```bash
git clone https://github.com/yourusername/DevOpsYoda.git
cd DevOpsYoda
pip install --break-system-packages -r requirements.txt
python3 app.py

ğŸŒ Deploy on Hugging Face (Free)
Go to https://huggingface.co/spaces

Create a new Space â†’ Choose Gradio

Clone the repo into the Space or push via:

git remote add space https://<token>@huggingface.co/spaces/<user>/DevOpsYoda
git push --force space main

Set your OPENAI_API_KEY or use a free LLM like flan-t5-xl

ğŸ“œ License
MIT License


